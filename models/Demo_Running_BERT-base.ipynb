{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e86783-2ab6-4503-8410-59c065c2077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig, Trainer\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "#from .drugs_condition_dictionary_finder import find_drugs_normalized_output, find_drugs_and_conditions_normalized_BIO_output\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "#from .bert_helper import get_label_list, tokenize_and_align_labels\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a10e9d-4f0a-4ecf-8f74-60425f680f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to help deal with Hugging Face BERT models\n",
    "# taken from https://github.com/michiyasunaga/LinkBERT/blob/main/src/tokcls/run_ner.py#L30\n",
    "# details on padding/truncation: https://huggingface.co/docs/transformers/pad_truncation\n",
    "import numpy as np\n",
    "\n",
    "def format_output_seqeval(results, return_format):\n",
    "    # see https://github.com/michiyasunaga/LinkBERT/blob/main/src/tokcls/run_ner.py#L30\n",
    "    if return_format == \"entity_level\":\n",
    "        # This is just flattening the result dict e.g. {'MISC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0,\n",
    "        # 'number': 1}, 'PER': {'precision': 1.0, 'recall': 0.5, 'f1': 0.66, 'number': 2}, 'overall_precision':\n",
    "        # 0.5, 'overall_recall': 0.33, 'overall_f1': 0.4, 'overall_accuracy': 0.66} -> {'MISC_precision': 0.0,\n",
    "        # 'MISC_recall': 0.0, 'MISC_f1': 0.0, 'MISC_number': 1, 'PER_precision': 1.0, 'PER_recall': 0.5,\n",
    "        # 'PER_f1': 0.66, 'PER_number': 2, 'overall_precision': 0.5, 'overall_recall': 0.33, 'overall_f1': 0.4,\n",
    "        # 'overall_accuracy': 0.66} Unpack nested dictionaries\n",
    "        final_results = {}\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, dict):\n",
    "                for n, v in value.items():\n",
    "                    final_results[f\"{key}_{n}\"] = v\n",
    "            else:\n",
    "                final_results[key] = value\n",
    "        return final_results\n",
    "    if return_format == \"macro\":\n",
    "        Ps, Rs, Fs = [], [], []\n",
    "        for type_name in results:\n",
    "            if type_name.startswith(\"overall\"):\n",
    "                continue\n",
    "            print('type_name', type_name)\n",
    "            Ps.append(results[type_name][\"precision\"])\n",
    "            Rs.append(results[type_name][\"recall\"])\n",
    "            Fs.append(results[type_name][\"f1\"])\n",
    "        return {\n",
    "            \"macro_precision\": np.mean(Ps),\n",
    "            \"macro_recall\": np.mean(Rs),\n",
    "            \"macro_f1\": np.mean(Fs),\n",
    "        }\n",
    "    if return_format == \"overall\":\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }\n",
    "    else:\n",
    "        return results\n",
    "\n",
    "def get_label_list(labels):\n",
    "    print(\"*** returning unique labels ***\")\n",
    "    unique_labels = set()\n",
    "    for label in labels:\n",
    "        unique_labels = unique_labels | set(label)\n",
    "    label_list = list(unique_labels)\n",
    "    label_list.sort()\n",
    "    return label_list\n",
    "\n",
    "# label_all_tokens: \"Whether to put the label for one word on all tokens of generated by that word\n",
    "# or just on the one (in which case the other tokens will have a padding index).\"\n",
    "def tokenize_and_align_labels(examples, tokenizer, text_column_name, label_column_name, label_to_id,\n",
    "                              label_all_tokens=False, padding=True, max_length=512):\n",
    "    print(\"*** Tokenize and Align Labels ***\")\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    word_ids_list = []\n",
    "    tokens_list = []\n",
    "    for i, label in enumerate(examples[label_column_name]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        tokenized_tokens = tokenized_inputs.tokens(batch_index=i)\n",
    "        word_ids_list.append(word_ids)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        tokens_sub = [] # want to find out which of the tokens will be evaluated at the end, taking word-piece tokenization into account\n",
    "        for i, word_idx in enumerate(word_ids):\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "                tokens_sub.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_to_id[label[word_idx]])\n",
    "                tokens_sub.append(tokenized_tokens[i])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label_to_id[label[word_idx]] if label_all_tokens else -100)\n",
    "                tokens_sub.append(tokenized_tokens[i] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "        tokens_list.append(tokens_sub)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    tokenized_inputs[\"word_ids\"] = word_ids_list\n",
    "    #tokenized_inputs[\"tokenized_tokens\"] = tokens_list # TODO: cauzes an ArrowInvalid error.\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8457806a-55eb-4b21-96f8-00664aac49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel:\n",
    "    def __init__(self, model_type, model_name, model_path=None, entity_class_names_dict=None):\n",
    "        self.model_name = model_name\n",
    "        if \"/\" in model_name:\n",
    "            self.model_name_short = self.model_name.split(\"/\")[1]\n",
    "        else:\n",
    "            self.model_name_short = self.model_name\n",
    "        self.model_path = model_path\n",
    "        self.model_type = model_type\n",
    "        self.return_words_only = False\n",
    "        self.entity_class_names_dict = entity_class_names_dict\n",
    "        self.normalize_pred_representation = True  # used for the hugging face models - it removes the additional\n",
    "        # information like prediction confidence that other models don't provide\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model_type == \"spacy\":\n",
    "            self.nlp = spacy.load(self.model_path)\n",
    "        elif self.model_type == \"huggingface\":\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = AutoModelForTokenClassification.from_pretrained(self.model_path)\n",
    "            self.config = AutoConfig.from_pretrained(self.model_path)\n",
    "            self.nlp = pipeline(\"ner\", model=self.model, tokenizer=self.tokenizer,\n",
    "                                grouped_entities=True)  # grouped entities False to analyze the tokenization of the models\n",
    "        elif self.model_type == \"regex\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Wrong model type. Allowed values are spacy, regex, and huggingface.\")\n",
    "\n",
    "    def bert_predict_bio_format(self, ds_path_train, ds_path_test, text_column_name, label_column_name,\n",
    "                                label_all_tokens=False, padding=True, max_length=512):\n",
    "        model = self.model\n",
    "        data_files = {\"train\": ds_path_train,\n",
    "                      \"test\": ds_path_test}\n",
    "        raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "        label_list = get_label_list(raw_datasets[\"train\"][label_column_name])\n",
    "        label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "        id_to_label = {i: l for i, l in enumerate(label_list)}\n",
    "\n",
    "        predict_dataset = raw_datasets[\"test\"]\n",
    "        predict_dataset = predict_dataset.map(\n",
    "            tokenize_and_align_labels,\n",
    "            batched=True,\n",
    "            desc=\"Running tokenizer on prediction dataset\",\n",
    "            fn_kwargs={\n",
    "                \"tokenizer\": self.tokenizer,\n",
    "                \"text_column_name\": text_column_name,\n",
    "                \"label_column_name\": label_column_name,\n",
    "                \"label_to_id\": label_to_id,\n",
    "                \"label_all_tokens\": label_all_tokens,\n",
    "                \"padding\": padding,\n",
    "                \"max_length\": max_length\n",
    "            }\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            train_dataset=None,\n",
    "            eval_dataset=None,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "\n",
    "        results = trainer.predict(predict_dataset)\n",
    "        predictions = results.predictions\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        mapped_list_of_predictions = [np.array([id_to_label[id] for id in arr]) for arr in predictions]\n",
    "\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            predict_dataset['tokens'],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "            is_split_into_words=True,\n",
    "        )\n",
    "        predict_dataset = predict_dataset.add_column(\"word_tokenized_input\",\n",
    "                                                     [tokenized_inputs.tokens(batch_index=i) for i in\n",
    "                                                      range(len(predict_dataset))])\n",
    "        predict_dataset_with_pred = predict_dataset.add_column(\n",
    "            \"predictions_bio_mapped_{}\".format(self.model_name_short), mapped_list_of_predictions)\n",
    "        predict_dataset_with_pred = predict_dataset_with_pred.add_column(\"predictions_{}\".format(self.model_name_short),\n",
    "                                                                         list(predictions))\n",
    "\n",
    "        return predict_dataset_with_pred\n",
    "\n",
    "    def annotate(self, file_path, source_column, sep=\",\"):\n",
    "        df = pd.read_csv(file_path, sep=sep)\n",
    "        if self.normalize_pred_representation:\n",
    "            predictions_col_name = \"ner_prediction_{}_normalized\".format(self.model_name_short)\n",
    "        else:\n",
    "            predictions_col_name = \"ner_prediction_{}\".format(self.model_name_short)\n",
    "        # df[predictions_col_name] = df[source_column].apply(self.infer_ner)\n",
    "        df[predictions_col_name] = df.apply(lambda row: self.infer_ner(row[source_column], row[\"tokens\"]),\n",
    "                                            axis=1)\n",
    "        return df\n",
    "\n",
    "    def annotate_parallel(self, file_path, source_column, sep=\",\"):\n",
    "        df = pd.read_csv(file_path, sep=sep)\n",
    "        num_processes = 10\n",
    "        chunks = [df[i:i + len(df) // num_processes] for i in\n",
    "                  range(0, len(df), len(df) // num_processes)]\n",
    "\n",
    "        with Pool(num_processes) as pool:\n",
    "            # Apply the function to each chunk in parallel\n",
    "            results = pool.starmap(self.infer_ner_chunk,\n",
    "                                   [(chunk, source_column) for chunk in chunks])\n",
    "        return pd.concat(results)\n",
    "\n",
    "    def infer_ner_chunk(self, chunk, text_source_column):\n",
    "        predictions_col_name = \"ner_prediction_{}\".format(self.model_name_short)\n",
    "        chunk[predictions_col_name] = chunk[text_source_column].apply(self.infer_ner)\n",
    "        if self.model_type == \"huggingface\":\n",
    "            self.normalize_pred_representation = True\n",
    "            predictions_col_name = \"ner_prediction_{}_normalized\".format(self.model_name_short)\n",
    "            chunk[predictions_col_name] = chunk[text_source_column].apply(self.infer_ner)\n",
    "            # predictions_col_name = \"ner_prediction_{}_bio\".format(self.model_type)\n",
    "            # chunk[predictions_col_name] = chunk[text_source_column].apply(self.ner_bert_bio_output)\n",
    "        return chunk\n",
    "\n",
    "    def ner_bert_bio_output(self, sentence):\n",
    "        # sentence = normalizer.normalize(sentence)\n",
    "\n",
    "        model, tokenizer, labels = self.model, self.tokenizer, list(self.config.label2id.keys())\n",
    "        #### check length limit ####\n",
    "        tokenized_sentence = self.tokenizer.tokenize(sentence)\n",
    "        num_tokens = len(tokenized_sentence)\n",
    "        if num_tokens > 512:\n",
    "            tokenized_sentence = tokenized_sentence[:510]\n",
    "            sentence = self.tokenizer.convert_tokens_to_string(tokenized_sentence)\n",
    "        #### check length limit ####\n",
    "\n",
    "        tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sentence)))\n",
    "        inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "        outputs = model(inputs)[0]\n",
    "        predictions = torch.argmax(outputs, axis=2)\n",
    "        predictions = [(token, labels[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())]\n",
    "\n",
    "        filtered_predictions = []\n",
    "        prev_token, prev_label = None, None\n",
    "\n",
    "        # dealing with the misaligned tokenizations\n",
    "        # TODO: seems too manual... trying to fit to the the prodigy tokenization here\n",
    "        for token, label in predictions:\n",
    "            if prev_token == 'cann' and token == '##ot' or (\n",
    "                    token == \"##mg\"):  # spacy splits connot into \"can\" and \"not\", two tokens\n",
    "                filtered_predictions.append(label)\n",
    "            elif token.startswith(\"##®\") or (not token.startswith('##') and token not in ['[SEP]',\n",
    "                                                                                          '[CLS]']):  # ignore the word pieces that would inflate the array\n",
    "                filtered_predictions.append(label)\n",
    "            prev_token, prev_label = token, label\n",
    "\n",
    "        return filtered_predictions\n",
    "\n",
    "    def infer_ner(self, sent, tokenized_sent=None):\n",
    "        if self.model_type == \"spacy\":\n",
    "            doc = self.nlp(sent)\n",
    "            entities = []\n",
    "            for ent in doc.ents:\n",
    "                if self.return_words_only:  # TODO where is the right place?\n",
    "                    entities.append(ent.text)\n",
    "                else:\n",
    "                    ent_label = ent.label_\n",
    "                    if ent_label == \"CHEMICAL\":  # TODO: handle multi-class case!\n",
    "                        entities.append((ent.start_char, ent.end_char, ent.text))\n",
    "            return list(set(entities))\n",
    "        elif self.model_type == \"huggingface\":\n",
    "            tokenized_sentence = self.tokenizer.tokenize(sent)\n",
    "            num_tokens = len(tokenized_sentence)\n",
    "            if num_tokens > 512:\n",
    "                print(\"Number of tokens ({}) too large, truncating sentence: {}\".format(num_tokens, sent[:50] + \"[...]\"))\n",
    "                tokenized_sentence = tokenized_sentence[:510]\n",
    "                sent = self.tokenizer.convert_tokens_to_string(tokenized_sentence)\n",
    "            ner_results = self.nlp(sent)\n",
    "            # ner_results_combined = self.combine_entity_subwords(sent, ner_results) NOT NEEDED IF WE ARE USING THE GROUPED ENTITIES FLAG\n",
    "            if self.normalize_pred_representation:\n",
    "                return self.normalize_representation(ner_results)\n",
    "            else:\n",
    "                return ner_results\n",
    "        elif self.model_type == \"regex\":\n",
    "            tokens_cleaned = tokenized_sent\n",
    "            return find_drugs_and_conditions_normalized_BIO_output(\n",
    "                tokens_cleaned)  # returns a tuple drug_matches, {\"entites\":all_char_indices}\n",
    "        else:\n",
    "            raise ValueError(\"Wrong model type. Allowed values are spacy and huggingface.\")\n",
    "\n",
    "    def normalize_representation(self, ner_results_combined):\n",
    "        results = []\n",
    "        for ent_dict in ner_results_combined:\n",
    "            if self.entity_class_names_dict:\n",
    "                entity_class_full_name = self.entity_class_names_dict[ent_dict['entity_group']]\n",
    "            else:\n",
    "                entity_class_full_name = ent_dict['entity_group']\n",
    "            results.append((ent_dict['start'], ent_dict['end'], entity_class_full_name, ent_dict['word']))\n",
    "        return results\n",
    "\n",
    "\n",
    "    def combine_entity_subwords(self, sent, entities):\n",
    "\n",
    "        result = []\n",
    "        current_entity = None\n",
    "        merged_dict = {}\n",
    "        if not entities:\n",
    "            return []\n",
    "\n",
    "        for entity in entities:\n",
    "            if entity[\"entity\"].startswith(\"B\") or entity[\"entity\"].startswith(\"I\"):\n",
    "                if current_entity is not None and entity[\"entity\"].startswith(\"I\"):\n",
    "                    if entity['word'].startswith('##'):\n",
    "                        current_entity['word'] += entity['word'][2:]\n",
    "                    elif current_entity['end'] == entity['start']:\n",
    "                        current_entity['word'] += entity['word']  # no space needed\n",
    "                    else:\n",
    "                        current_entity['word'] += \" \"\n",
    "                        current_entity['word'] += entity['word']\n",
    "                    current_entity['end'] = entity['end']\n",
    "                    current_entity['score'] = round((current_entity['score'] + entity['score']) / 2,\n",
    "                                                    3)  # average of the confidence\n",
    "                else:\n",
    "                    current_entity = entity.copy()\n",
    "                    current_entity['entity'] = entity['entity'].replace(\"B-\", \"\").replace(\"I-\",\n",
    "                                                                                          \"\")  # TODO: does the I- make sense?\n",
    "                    if self.entity_class_names_dict:\n",
    "                        current_entity['entity'] = self.entity_class_names_dict[current_entity['entity']]\n",
    "                    current_entity['word'] = entity['word'][2:] if entity['word'].startswith('##') else entity['word']\n",
    "                    result.append(current_entity)\n",
    "\n",
    "        if self.return_words_only:\n",
    "            current_entity_start = 0\n",
    "            for i, entity in enumerate(result):\n",
    "\n",
    "                if entity['entity'] == 'B':\n",
    "                    merged_dict[i] = entity['word']\n",
    "                    current_entity_start = i\n",
    "                elif entity['entity'] == 'I':\n",
    "                    if merged_dict:\n",
    "                        merged_dict[current_entity_start] += ' ' + entity['word']\n",
    "                    else:\n",
    "                        print(\"could not save entity: {} from sentence {}. All entities found: {}\".format(entity, sent,\n",
    "                                                                                                          entities))\n",
    "\n",
    "            return list(merged_dict.values())\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ab8cd50-21a9-48b2-9022-cd8865a6ad82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERT-BASE model_annotations.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad04526e79764b57aba41640ccb68a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a15fe7f0a6c4a268ba1826e7994088b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc6b0c4df454373830f1b20c57976df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3397c0c2430743cba35580545973a0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** returning unique labels ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0afe3387a8458db158ef68e877388b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on prediction dataset:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Tokenize and Align Labels ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18bd3c77729246ebbe781b12dd9d5e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from core.models import NERModel\n",
    "import datetime\n",
    "\n",
    "short_to_long_class_names_map = {\n",
    "    \"DRUG\": \"DRUG\",\n",
    "    \"BEH\": \"BEHAVIOURAL\",\n",
    "    \"SURG\": \"SURGICAL\",\n",
    "    \"PHYS\": \"PHYSICAL\",\n",
    "    \"RADIO\": \"RADIOTHERAPY\",\n",
    "    \"OTHER\": \"OTHER\",\n",
    "    \"COND\": \"CONDITION\",\n",
    "    \"CTRL\": \"CONTROL\"\n",
    "}\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "run_linkbert = False\n",
    "run_biobert = False\n",
    "run_bert_base_uncased = True\n",
    "\n",
    "# TODO: Error \"Placeholder storage has not been allocated on MPS device!\" when trying to run tuple and BIO annotations sequentially?\n",
    "run_tuples_annotations = False\n",
    "run_BIO_annotations = True\n",
    "\n",
    "relevant_data_path = \"../data/annotated_data/\"\n",
    "corpus_files_path_prefix = relevant_data_path + \"data_splits/\"\n",
    "train_data_path = corpus_files_path_prefix + \"ct_neuro_train_data_713.json\"\n",
    "test_data_path = corpus_files_path_prefix + \"ct_neuro_test_data_90.json\"\n",
    "test_data_path_csv = corpus_files_path_prefix + \"ct_neuro_test_merged_90.csv\"\n",
    "output_annotations_path_prefix = \"./predictions/\"\n",
    "\n",
    "#### BERT BASE ####\n",
    "if run_bert_base_uncased:\n",
    "    print(\"Running BERT-BASE model_annotations.\")\n",
    "    hugging_face_model_name = \"bert-base-uncased\"\n",
    "    hugging_face_model_path = \"./bert/trained/bert-base-uncased/epochs_15_data_size_100_iter_4/\"\n",
    "    model_name_str = \"bert-base-uncased\"\n",
    "    model = NERModel(\"huggingface\", hugging_face_model_name, hugging_face_model_path, short_to_long_class_names_map)\n",
    "\n",
    "    ### ANNOTATE WITH BIO OUTPUT\n",
    "    if run_BIO_annotations:\n",
    "        predict_dataset_with_pred = model.bert_predict_bio_format(train_data_path, test_data_path, \"tokens\",\n",
    "                                                                  \"ner_tags\")\n",
    "        predict_dataset_with_pred.to_csv(\n",
    "            output_annotations_path_prefix + \"ct_neuro_test_annotated_{}_BIO_{}_test.csv\".format(model_name_str,\n",
    "                                                                                            current_date), sep=\",\")\n",
    "    if run_tuples_annotations:\n",
    "        ### ANNOTATE WITH TUPLE OUTPUT\n",
    "        annotated_ds = model.annotate(test_data_path_csv, \"text\")\n",
    "        annotated_ds.to_csv(\n",
    "            output_annotations_path_prefix + \"ct_neuro_test_annotated_{}_{}_test.csv\".format(model_name_str,\n",
    "                                                                                        current_date), sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdaad9bf-c978-44bf-882e-79818f793da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1be01d66-1be0-4895-8f3a-078857d05c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bert_bio(target_annotated_file_name, train_file_name, train_label_column_name,\n",
    "                      return_format=\"all\", target_labels_column=\"labels\", predicted_labels_column=\"predictions\"):\n",
    "    df = pd.read_csv(target_annotated_file_name)\n",
    "\n",
    "    def convert_to_list(string):\n",
    "        string = string.strip('[]')  # Remove the brackets\n",
    "        return list(map(int, string.split()))\n",
    "\n",
    "    predictions = np.array(df[predicted_labels_column].apply(convert_to_list).to_list())\n",
    "    labels = np.array(df[target_labels_column].apply(convert_to_list).to_list())\n",
    "\n",
    "    data_files = {\"train\": train_file_name}\n",
    "    raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "    label_list = get_label_list(raw_datasets[\"train\"][train_label_column_name])\n",
    "\n",
    "    print(\"predicted_labels_column: \", predicted_labels_column)\n",
    "    print(\"len: \", len(predictions))\n",
    "    print(\"target_labels_column: \", target_labels_column)\n",
    "    print(\"len: \", len(labels))\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    metric = load_metric(\"seqeval\")\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "\n",
    "    combined_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "    combined_target = [item for sublist in true_labels for item in sublist]\n",
    "    #self.calculate_overall_cohen_kappa_with_ci(combined_predictions, combined_target)\n",
    "\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(true_labels, true_predictions))\n",
    "    print(\"Evaluation Mode STRICT\")\n",
    "    print(classification_report(true_labels, true_predictions, mode='strict', scheme=IOB2))\n",
    "\n",
    "    return format_output_seqeval(results, return_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d6f4730-910d-4ed1-a35e-0e655523aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_str = \"bert-base-uncased\"\n",
    "annotated_data_path = output_annotations_path_prefix + \"ct_neuro_test_annotated_{}_{}_test.csv\".format(model_name_str, \"20240131\")\n",
    "annotated_data_path_bio = output_annotations_path_prefix + \"ct_neuro_test_annotated_{}_BIO_{}_test.csv\".format(model_name_str, \"20240131\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cdf6682-e48c-4f3e-8879-bd34e8de491e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** returning unique labels ***\n",
      "predicted_labels_column:  predictions_bert-base-uncased\n",
      "len:  90\n",
      "target_labels_column:  labels\n",
      "len:  90\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         BEH       0.23      0.15      0.18        48\n",
      "        COND       0.77      0.78      0.78       468\n",
      "        CTRL       0.54      0.50      0.52        38\n",
      "        DRUG       0.72      0.88      0.79       128\n",
      "       OTHER       0.33      0.41      0.37       134\n",
      "        PHYS       0.51      0.39      0.44        66\n",
      "       RADIO       0.00      0.00      0.00         1\n",
      "        SURG       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.64      0.66      0.65       883\n",
      "   macro avg       0.39      0.39      0.38       883\n",
      "weighted avg       0.64      0.66      0.65       883\n",
      "\n",
      "Evaluation Mode STRICT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         BEH       0.32      0.15      0.20        48\n",
      "        COND       0.79      0.78      0.79       468\n",
      "        CTRL       0.58      0.50      0.54        38\n",
      "        DRUG       0.74      0.86      0.79       128\n",
      "       OTHER       0.39      0.40      0.40       134\n",
      "        PHYS       0.63      0.36      0.46        66\n",
      "       RADIO       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.69      0.66      0.67       883\n",
      "   macro avg       0.49      0.44      0.45       883\n",
      "weighted avg       0.67      0.66      0.66       883\n",
      "\n",
      "{'BEH': {'precision': 0.22580645161290322, 'recall': 0.14583333333333334, 'f1': 0.17721518987341772, 'number': 48}, 'COND': {'precision': 0.7710084033613446, 'recall': 0.7841880341880342, 'f1': 0.777542372881356, 'number': 468}, 'CTRL': {'precision': 0.5428571428571428, 'recall': 0.5, 'f1': 0.5205479452054795, 'number': 38}, 'DRUG': {'precision': 0.717948717948718, 'recall': 0.875, 'f1': 0.7887323943661971, 'number': 128}, 'OTHER': {'precision': 0.32934131736526945, 'recall': 0.41044776119402987, 'f1': 0.3654485049833887, 'number': 134}, 'PHYS': {'precision': 0.5098039215686274, 'recall': 0.3939393939393939, 'f1': 0.4444444444444444, 'number': 66}, 'RADIO': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1}, 'SURG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}, 'overall_precision': 0.6369565217391304, 'overall_recall': 0.6636466591166478, 'overall_f1': 0.6500277315585137, 'overall_accuracy': 0.9363052512278051}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donevas/opt/miniconda3/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/donevas/opt/miniconda3/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_bert_bio(annotated_data_path_bio, \"../data/annotated_data/data_splits/ct_neuro_train_data_713.json\",\n",
    "                                          \"ner_tags\", return_format=\"all\", target_labels_column=\"labels\",\n",
    "                                          predicted_labels_column=f\"predictions_{model_name_str}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c6378-8503-4ed8-8b49-2d0f7fa3ed9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
